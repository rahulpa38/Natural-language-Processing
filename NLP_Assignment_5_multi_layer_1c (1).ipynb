{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "UNwdZ4bpca3n",
    "outputId": "5340ab48-5aa7-47da-a7ef-f39b81c0f007"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZqQ5A0MclWq"
   },
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Uvcmo_t5cskd",
    "outputId": "3d9ca5f5-ffbc-4bcc-835e-ed06c84b6f83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "qRdod6GOdYvU",
    "outputId": "f4659991-40a3-4c3f-c4c7-45e830d607d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## First 250 characters \n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9tC7uYqPdh-j",
    "outputId": "f3c3b9e9-511c-4b8a-b21a-2cdbec84cdcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JCgbqEHddngR"
   },
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szTiKhEydqYD"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gVRHfeQcduKS"
   },
   "outputs": [],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GG8JE-jQdv9v",
    "outputId": "94f0992a-44a8-4f50-f1b6-c839df8423bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMZ_XWvMePGy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "HXB0e2O9dzMS",
    "outputId": "8a6eff79-fbb0-4e0d-c0cf-699910283292"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "RGqGPNT9d9ap",
    "outputId": "c7a1dbae-4cba-443e-af92-63cb4ea72259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f6Jrv3ISiwoM",
    "outputId": "76cd9be1-3392-43ec-c0ad-53de233c2af6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (101,), types: tf.int64>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X9GVzF02eaBC"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "osBpIKL5i1sn",
    "outputId": "6d87a956-5421-4058-e7c5-cbe02293ffc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "P3uae-_Veb6a",
    "outputId": "4fae96a4-6528-4f93-b590-e248a4e06beb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "q3frf5iFedl8",
    "outputId": "6c8ab3ec-4009-47fd-8693-f0944c60eec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 18 ('F')\n",
      "  expected output: 47 ('i')\n",
      "Step    1\n",
      "  input: 47 ('i')\n",
      "  expected output: 56 ('r')\n",
      "Step    2\n",
      "  input: 56 ('r')\n",
      "  expected output: 57 ('s')\n",
      "Step    3\n",
      "  input: 57 ('s')\n",
      "  expected output: 58 ('t')\n",
      "Step    4\n",
      "  input: 58 ('t')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4n8_41Kaefii",
    "outputId": "aad99bc8-403f-4340-df30-03c2459d61d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-j_Ywk0peh4W"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bW3Gg-3xejlP"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "  \n",
    "  tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "  tf.keras.layers.GRU(rnn_units,\n",
    "                        dropout = 0.5,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "  tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),                        \n",
    "  tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Um3YhdbNexWc"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ec3--abEeyzD",
    "outputId": "bdcce079-3db9-4e58-9611-66ceda434c77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "Q6auy6V3e3FY",
    "outputId": "5a36281e-66f2-43de-d8e3-857333813f64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3935232   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (64, None, 1024)          6294528   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 10,313,025\n",
      "Trainable params: 10,313,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z1h6nk8ge9k6"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "jwrPeUAhfBOl",
    "outputId": "92aea779-5eb8-4e7a-c46d-2bc385ffa820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'ight feel your love\\nThan my unpleased eye see your courtesy.\\nUp, cousin, up; your heart is up, I kno'\n",
      "Next Char Predictions: \n",
      " \"vdyXK-YTxcDiUMusUQCAlAUlHl-QU.pI-YT$WhTa&C'GsFR DIJ'KKo.&,HbSSAJS-NokmO;jMYD!-OXufDx!XIOuR3H&g$\\nyvLm\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "syiU0K-OfDLP",
    "outputId": "e0a398a6-cc92-4820-f7ad-aed774c83ace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.1742425\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_A8FiV2fKV5"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RCUCSuANfL_P"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RiYOwLy5fNPO"
   },
   "outputs": [],
   "source": [
    "EPOCHS=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "X2PtW2IpfOes",
    "outputId": "4982f0e9-1186-4b3c-a0a5-0c7d14dc170a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "172/172 [==============================] - 200s 1s/step - loss: 2.7015\n",
      "Epoch 2/30\n",
      "172/172 [==============================] - 195s 1s/step - loss: 1.9190\n",
      "Epoch 3/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 1.6406\n",
      "Epoch 4/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 1.5037\n",
      "Epoch 5/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 1.4237\n",
      "Epoch 6/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 1.3646\n",
      "Epoch 7/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 1.3162\n",
      "Epoch 8/30\n",
      "172/172 [==============================] - 197s 1s/step - loss: 1.2704\n",
      "Epoch 9/30\n",
      "172/172 [==============================] - 197s 1s/step - loss: 1.2245\n",
      "Epoch 10/30\n",
      "172/172 [==============================] - 197s 1s/step - loss: 1.1789\n",
      "Epoch 11/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 1.1317\n",
      "Epoch 12/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 1.0875\n",
      "Epoch 13/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 1.0471\n",
      "Epoch 14/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 1.0083\n",
      "Epoch 15/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 0.9701\n",
      "Epoch 16/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 0.9281\n",
      "Epoch 17/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 0.8886\n",
      "Epoch 18/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 0.8558\n",
      "Epoch 19/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 0.8261\n",
      "Epoch 20/30\n",
      "172/172 [==============================] - 195s 1s/step - loss: 0.7967\n",
      "Epoch 21/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 0.7663\n",
      "Epoch 22/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 0.7328\n",
      "Epoch 23/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 0.7070\n",
      "Epoch 24/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 0.6809\n",
      "Epoch 25/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 0.6576\n",
      "Epoch 26/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 0.6364\n",
      "Epoch 27/30\n",
      "172/172 [==============================] - 195s 1s/step - loss: 0.6192\n",
      "Epoch 28/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 0.5963\n",
      "Epoch 29/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 0.5806\n",
      "Epoch 30/30\n",
      "172/172 [==============================] - 196s 1s/step - loss: 0.5571\n"
     ]
    }
   ],
   "source": [
    "#@title Default title text\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H8yC74IlfPSo",
    "outputId": "476f49bc-e379-4fba-9c3e-be5decf28ffc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_30'"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_ checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JpcX1S1i7hV1"
   },
   "outputs": [],
   "source": [
    "model = build_model(  vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5XPC8Jmj7m2I"
   },
   "outputs": [],
   "source": [
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "\n",
    "def generate_text(model, start_string,temperature):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "GHhGvkjJ8ONu",
    "outputId": "b78536a0-1b51-4e3f-de88-3f0b9549b490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: if it be\n",
      "not--\n",
      "Frant Petruchio?' will to keep the traitor's hand,\n",
      "Inten it press'd for peace: the yoke\n",
      "on him edge our airy reigns in meeting kis; which rouses upon woond:\n",
      "O, but go enter it before him; that\n",
      "We would have understand moves me: thou know'st\n",
      "With ons the bloody friend we enjoy, my grave,\n",
      "That we no shameless ripes grown into arms:\n",
      "If you so honour and Vours,\n",
      "Bearing a bawd, plant, boats the city fellow;\n",
      "Or else impatient that hath dispersed that merrily\n",
      "instire to come to them at hand: it waughty brother-day:\n",
      "What, Warwick!\n",
      "Call forth! tell him I hear,\n",
      "You, that my wife's a beard take when they are great with\n",
      "trail. And but to end a duke that you mad.\n",
      "\n",
      "PETRUCHIO:\n",
      "Signior Hortensio, 'twixs my grand ir't\n",
      "Hat thou coward match, sirray libble; then wish this\n",
      "blood whit thou rest.\n",
      "\n",
      "KING RICHARD III:\n",
      "Except, though I owe thou wot well wrong,\n",
      "And therefore fellow the wower of little\n",
      "of a doter patient.\n",
      "\n",
      "BAPTISTA:\n",
      "Fie, fie! understand him for the people.\n",
      "\n",
      "CORIOLANUS:\n",
      "Scorpan him \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \",temperature = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "colab_type": "code",
    "id": "XENuSjvyIc-Z",
    "outputId": "afb11bff-4902-4e92-b60b-b115b90dcbd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: if it be so? God have fivol had a sword, and\n",
      "'tis well; and Romeo will were traitor,\n",
      "My mind is chanced to be endured!\n",
      "Why, then I hear, Queen and the Tower,\n",
      "OffEd whom thou hast found it, his enters no time.\n",
      "\n",
      "MIRANDA:\n",
      "You have fought so long such heaven's ignoralt rest!\n",
      "Carnals forth that the rest from out a good lurge.\n",
      "\n",
      "KING RICHARD II:\n",
      "Well, madam, 'tis no time to chide.\n",
      "\n",
      "JULIET:\n",
      "I think be so, it is my father's moies.\n",
      "God save thy heart!\n",
      "\n",
      "POLIXENES:\n",
      "We warrant,\n",
      "With this immodness, though I wish thy life and doubt.\n",
      "\n",
      "YORK:\n",
      "Son, I'll curdener.\n",
      "\n",
      "POLIXENES:\n",
      "Here, or else thou canst not fear them.\n",
      "\n",
      "MISTRESS OVERDONE:\n",
      "What mean these majesty may men, whole heavy fond\n",
      "That time I'ld give thee in my ears, to misistery.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "KING EDWARD IV:\n",
      "So, madam.\n",
      "\n",
      "VIRGILIA:\n",
      "O, too fondly meet!\n",
      "\n",
      "First Murderer:\n",
      "Take that, and would; a taport, stirring unseen\n",
      "The corsol black instelleg, rash lost itself,\n",
      "Let him be sect\n",
      "Of this dishonour'd by out of them; yes, here are enter'd in the city,\n",
      "Whils\n"
     ]
    }
   ],
   "source": [
    "## changing the temperature to 0.7\n",
    "print(generate_text(model, start_string=u\"ROMEO: \",temperature = 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "pdoTH_tSIinz",
    "outputId": "9c2d1eec-bcb5-4be6-c505-13258aecdb76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: my wife?\n",
      "\n",
      "Captain:\n",
      "My lord, your brother is imprison'd by the nakeo:\n",
      "It is a mirrous for mouth and the sword of his hasty more death,\n",
      "Which I did think that they shall know me,\n",
      "Let one sent forth his head into a most holy sir,\n",
      "Whilst you shall come to your shepherd. The time is very short.\n",
      "\n",
      "PERDITA:\n",
      "I'll not be the first that we have seen vows and blood\n",
      "With her most sweet, nor do't of this:\n",
      "What, ho! my lord, the king of war,\n",
      "Than fiery eyes'd that we banished his lip\n",
      "Hath dimm'd your itself through moved his hands\n",
      "As occused him and his cheeks from\n",
      "The worthiest tender your estate, and here\n",
      "I\n",
      "For the benefit of a ground died,\n",
      "And only chat answer'd him, the selfsame feat\n",
      "In this before: that are thou must be abused\n",
      "'I fear, though long, our joys with slower for\n",
      "the proudest here to make her goodly things as you!\n",
      "\n",
      "Both Citizens:\n",
      "The climate he is of little upon't.\n",
      "\n",
      "MENENIUS:\n",
      "Why, 'tis not my wife; you fear, I'll go along with thee:\n",
      "Farewell, and stir to fly,\n",
      "Whilst thou neglect'st or \n"
     ]
    }
   ],
   "source": [
    "## changing the temperature to 0.7\n",
    "print(generate_text(model, start_string=u\"ROMEO: \", temperature = 0.5 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsQvieTZSTCZ"
   },
   "source": [
    "# Here I have increased the number of layers of gru as more number of neurons are learning loss is decreasing and network is able to more from the data as epoch progresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Assignment-5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
